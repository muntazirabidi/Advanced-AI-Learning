# Home.py
import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from utils.plotting import create_theme_plot
from utils.distributions import plot_beta_distribution

def main():
    st.set_page_config(
        page_title="Learn Bayesian Inference",
        page_icon="ðŸ“Š",
        layout="wide"
    )
    
    st.title("ðŸ“š Interactive Bayesian Inference Learning")
    
    st.markdown("""
    Welcome to your journey into Bayesian Statistics! This interactive application
    will help you understand the fundamental concepts of Bayesian inference through
    visual demonstrations and hands-on examples.
    
    ### What you'll learn:
    1. Prior and Likelihood
    2. Conjugate Priors
    3. MCMC Sampling
    """)

    # Theoretical Foundation Section
    st.header("ðŸ“– Theoretical Foundations of Bayesian Inference")
    
    st.markdown("""
    ### The Heart of Bayesian Inference
    
    Bayesian inference represents a powerful framework for updating our beliefs based on evidence. At its core lies Bayes' Theorem:

    $$ P(Î¸|D) = \\frac{P(D|Î¸)P(Î¸)}{P(D)} $$

    Where:
    - $P(Î¸|D)$ is the **posterior probability** - our updated belief about parameter Î¸ after seeing data D
    - $P(D|Î¸)$ is the **likelihood** - the probability of observing data D given parameter Î¸
    - $P(Î¸)$ is the **prior probability** - our initial belief about parameter Î¸
    - $P(D)$ is the **evidence** - the total probability of observing data D
    
    ### Understanding Each Component

    #### 1. Prior Probability
    The prior probability represents our initial beliefs before seeing any data. It can be:
    - **Informative**: Based on previous studies or expert knowledge
    - **Uninformative**: When we want to express minimal prior knowledge
    - **Conjugate**: Chosen to make posterior calculations mathematically convenient
    
    For example, a Beta distribution is often used as a prior for probability parameters because:
    $$ P(Î¸) = \\frac{Î¸^{Î±-1}(1-Î¸)^{Î²-1}}{B(Î±,Î²)} $$
    where B(Î±,Î²) is the Beta function.

    #### 2. Likelihood Function
    The likelihood represents how probable our observed data is under different parameter values:
    $$ L(Î¸;D) = P(D|Î¸) $$
    
    For example, in a coin-flipping experiment with n trials and k successes:
    $$ P(D|Î¸) = \\binom{n}{k}Î¸^k(1-Î¸)^{n-k} $$

    #### 3. Posterior Distribution
    The posterior combines our prior beliefs with the evidence from our data:
    $$ P(Î¸|D) âˆ P(D|Î¸)P(Î¸) $$
    
    In many cases, we can write this as:
    $$ \text{Posterior} âˆ \text{Likelihood} Ã— \text{Prior} $$
    """)

    # Interactive Visualization Section
    st.header("ðŸ”„ Interactive Beta Distribution")
    st.markdown("""
    The Beta distribution plays a central role in Bayesian inference because:
    1. It's defined on the interval [0,1], making it perfect for probability parameters
    2. It's conjugate to the Binomial likelihood
    3. Its parameters have intuitive interpretations:
       - Î± represents the number of "successes" plus 1
       - Î² represents the number of "failures" plus 1
    """)
    
    col1, col2 = st.columns(2)
    
    with col1:
        prior_alpha = st.slider("Î± (Alpha) parameter", 0.1, 10.0, 2.0, 0.1,
                              help="Controls the concentration of probability mass above 0.5")
        prior_beta = st.slider("Î² (Beta) parameter", 0.1, 10.0, 2.0, 0.1,
                             help="Controls the concentration of probability mass below 0.5")
        
        st.markdown(f"""
        **Current Distribution Properties:**
        - Mean: {prior_alpha/(prior_alpha + prior_beta):.3f}
        - Mode: {(prior_alpha - 1)/(prior_alpha + prior_beta - 2):.3f} (when Î±,Î² > 1)
        - Variance: {(prior_alpha * prior_beta)/((prior_alpha + prior_beta)**2 * (prior_alpha + prior_beta + 1)):.3f}
        """)
    
    with col2:
        fig = plot_beta_distribution(prior_alpha, prior_beta)
        st.pyplot(fig)
        plt.close()
    
    st.markdown("""
    ### Understanding Shape Parameters
    
    The shape of the Beta distribution is determined by its two parameters:
    
    - When Î± = Î² = 1: Uniform distribution (completely uncertain)
    - When Î±, Î² > 1: Unimodal (single peak)
    - When Î±, Î² < 1: U-shaped (peaks at 0 and 1)
    - When Î± = Î²: Symmetric around 0.5
    - Large Î±, Î²: More concentrated distribution
    
    Try adjusting the parameters to see how they affect the distribution's shape!
    """)

    # Navigation Guide
    st.header("ðŸ—ºï¸ Navigation Guide")
    st.markdown("""
    Continue your learning journey through the following pages:
    
    1. **Prior & Likelihood**: Understand how these components interact
    2. **Conjugate Priors**: Learn about mathematical convenience in Bayesian updating
    3. **MCMC Sampling**: Explore methods for complex posterior distributions
    
    Each section builds upon the concepts introduced here, providing deeper insights
    and more advanced applications of Bayesian inference.
    """)

if __name__ == "__main__":
    main()