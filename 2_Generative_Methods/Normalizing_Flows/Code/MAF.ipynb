{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Basic Masked Autoregressive Flow (MAF) in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by importing PyTorch and necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Masked Linear Layer\n",
    "In an autoregressive model, we need to ensure that outputs do not depend on \"future\" inputs. A custom MaskedLinear layer multiplies its weights by a binary mask to enforce this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MaskedLinear, self).__init__(in_features, out_features, bias)\n",
    "        # Initialize the mask as ones. We'll set it later.\n",
    "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
    "        \n",
    "    def set_mask(self, mask):\n",
    "        # Copy the provided mask into our buffer.\n",
    "        self.mask.data.copy_(mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply the mask to the weights before performing the linear transformation.\n",
    "        return F.linear(x, self.weight * self.mask, self.bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- MaskedLinear extends nn.Linear.\n",
    "- It holds a mask (binary matrix) that is applied element-wise to the weights before the linear transformation.\n",
    "- This mask will be set later to enforce that each output only depends on a subset of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a MADE Block\n",
    "\n",
    "A MADE (Masked Autoencoder for Distribution Estimation) block uses masked layers to model an autoregressive factorization of the joint distribution. Each MADE outputs parameters (e.g., shift and log-scale) for each input variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MADE(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Number of input features (and output variables).\n",
    "            hidden_size: Number of units in the hidden layer.\n",
    "            output_size: Typically 2 * input_size (for shift and log-scale per variable).\n",
    "        \"\"\"\n",
    "        super(MADE, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Define two masked linear layers: input -> hidden, then hidden -> output.\n",
    "        self.fc1 = MaskedLinear(input_size, hidden_size)\n",
    "        self.fc2 = MaskedLinear(hidden_size, output_size)\n",
    "\n",
    "        # Create the masks to enforce the autoregressive property.\n",
    "        self.create_masks()\n",
    "\n",
    "    def create_masks(self):\n",
    "        # Define degrees for input neurons: here we assume a natural ordering.\n",
    "        input_degrees = torch.arange(1, self.input_size + 1)\n",
    "\n",
    "        # Assign random degrees to hidden neurons between 1 and input_size - 1.\n",
    "        hidden_degrees = torch.randint(1, self.input_size, (self.hidden_size,))\n",
    "\n",
    "        # For the output, repeat the input degrees to match the output size.\n",
    "        output_degrees = input_degrees.repeat(self.output_size // self.input_size)\n",
    "\n",
    "        # Create the mask for the first layer:\n",
    "        # Allow connection if hidden_degree >= input_degree.\n",
    "        mask1 = (hidden_degrees.unsqueeze(1) >= input_degrees.unsqueeze(0)).float()\n",
    "        self.fc1.set_mask(mask1)\n",
    "\n",
    "        # Create the mask for the second layer:\n",
    "        # Allow connection if output_degree > hidden_degree.\n",
    "        mask2 = (output_degrees.unsqueeze(1) > hidden_degrees.unsqueeze(0)).float()\n",
    "        self.fc2.set_mask(mask2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through first masked layer with ReLU activation.\n",
    "        h = F.relu(self.fc1(x))\n",
    "        # The output layer provides parameters (e.g., mu and log_scale for each variable).\n",
    "        out = self.fc2(h)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "\n",
    "- **Degrees:** We assign an ordering (or “degree”) to inputs, hidden, and output neurons.\n",
    "- **Masking:**\n",
    "    - For fc1, a connection is allowed if the hidden neuron’s degree is greater than or equal to the input neuron’s degree.\n",
    "    - For fc2, a connection is allowed if the output neuron’s degree is greater than the hidden neuron’s degree.\n",
    "- **Output:** The MADE block produces a vector that is typically split into two parts (shift and log-scale) for each input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MAF by Stacking MADE Blocks\n",
    "A MAF (Masked Autoregressive Flow) stacks multiple MADE blocks. Each block transforms the input while accumulating the log-determinant of the Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAF(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size: Dimensionality of the data.\n",
    "            hidden_size: Number of hidden units per MADE block.\n",
    "            n_layers: Number of MADE blocks to stack.\n",
    "        \"\"\"\n",
    "        super(MAF, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        # Create a ModuleList of MADE blocks.\n",
    "        self.layers = nn.ModuleList([\n",
    "            MADE(input_size, hidden_size, input_size * 2)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize the log-determinant of the Jacobian.\n",
    "        log_det = torch.zeros(x.size(0), device=x.device)\n",
    "        # Pass the data sequentially through each MADE block.\n",
    "        for layer in self.layers:\n",
    "            # Each layer outputs a vector which we split into shift (mu) and log-scale (log_scale).\n",
    "            out = layer(x)\n",
    "            mu, log_scale = out.chunk(2, dim=1)\n",
    "            # Apply the affine transformation:\n",
    "            # x_new = (x - mu) * exp(-log_scale)\n",
    "            x = (x - mu) * torch.exp(-log_scale)\n",
    "            # Update the log-determinant (negative log_scale contributes additively).\n",
    "            log_det -= log_scale.sum(dim=1)\n",
    "        return x, log_det\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation:\n",
    "\n",
    "- Stacking: We create several MADE blocks stored in a ModuleList.\n",
    "- Transformation: Each block applies an affine transformation to x using the outputs of the MADE block (shift and scale).\n",
    "- Jacobian: The log-determinant of the Jacobian is accumulated across layers. This is required for calculating the likelihood during training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage of the MAF\n",
    "Finally, we create an instance of the MAF, feed in some dummy data, and inspect the output and log-determinant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input x:\n",
      "tensor([[-0.5727, -2.4861],\n",
      "        [-0.7693, -0.7929],\n",
      "        [ 0.0873, -0.2468],\n",
      "        [-2.0814, -0.5374],\n",
      "        [-0.8872, -0.1615]])\n",
      "\n",
      "Transformed z (latent representation):\n",
      "tensor([[-0.4669, -2.3552],\n",
      "        [-0.5883, -0.8918],\n",
      "        [-0.0591, -0.4736],\n",
      "        [-1.3990, -0.3023],\n",
      "        [-0.6612, -0.3560]], grad_fn=<MulBackward0>)\n",
      "\n",
      "Log-determinant of the Jacobian:\n",
      "tensor([-0.6593, -0.7061, -0.6280, -0.9459, -0.7251], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensionality of our data.\n",
    "input_size = 2      # For example, 2-dimensional data\n",
    "hidden_size = 10    # Number of hidden units in each MADE block\n",
    "n_layers = 3        # Number of MADE blocks to stack\n",
    "\n",
    "# Instantiate the MAF model.\n",
    "maf = MAF(input_size, hidden_size, n_layers)\n",
    "\n",
    "# Create a batch of random data (e.g., 5 samples).\n",
    "x = torch.randn(5, input_size)\n",
    "\n",
    "# Pass the data through the MAF.\n",
    "z, log_det = maf(x)\n",
    "\n",
    "print(\"Input x:\")\n",
    "print(x)\n",
    "print(\"\\nTransformed z (latent representation):\")\n",
    "print(z)\n",
    "print(\"\\nLog-determinant of the Jacobian:\")\n",
    "print(log_det)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrated a basic implementation of a Masked Autoregressive Flow (MAF) by building custom masked layers, a MADE block, and then stacking these to form a flow. This structure is useful for transforming a simple base distribution into a complex target distribution—an essential step in simulation-based inference.\n",
    "\n",
    "Feel free to modify the architecture or experiment with different parameters to further your understanding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
